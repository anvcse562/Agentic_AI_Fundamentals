import json
import time
import os
from typing import List, TypedDict, Dict, Any, Optional

# --- NEW IMPORTS FOR OPENAI ---
try:
    from openai import OpenAI
    from dotenv import load_dotenv
except ImportError:
    print("Please install required packages: pip install openai python-dotenv")
    exit(1)

# =============================================================================
# SETUP: OPENAI CLIENT
# =============================================================================
# Load environment variables from a .env file
load_dotenv()

# Initialize Client (ensure OPENAI_API_KEY is in your .env file or environment)
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    print("WARNING: OPENAI_API_KEY not found. Please set it in your .env file.")
    # Fallback for demonstration if no key exists (prevents crash on run)
    client = None
else:
    client = OpenAI(api_key=api_key)

# =============================================================================
# HELPER: LLM WRAPPER
# =============================================================================
def invoke_llm(system_prompt: str, user_prompt: str, model: str = "gpt-4o", json_mode: bool = False) -> str:
    """
    Wrapper for OpenAI API calls. 
    Handles basic configuration and JSON formatting if requested.
    """
    if not client:
        return "Error: No API Key provided."

    print(f"   [LLM ({model})]: Processing...")
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    kwargs = {
        "model": model,
        "messages": messages,
        "temperature": 0.7
    }
    
    # Enforce JSON mode if requested (Supported on gpt-4o, gpt-3.5-turbo-0125)
    if json_mode:
        kwargs["response_format"] = {"type": "json_object"}

    try:
        response = client.chat.completions.create(**kwargs)
        content = response.choices[0].message.content
        return content
    except Exception as e:
        print(f"   [API Error]: {e}")
        return ""

# =============================================================================
# TOPIC 2 & 3: ROBUST STATEFUL SINGLE AGENT & PLANNING PATTERN
# =============================================================================

# 1. Define the State Schema
class PlanState(TypedDict):
    destination: str
    budget: int
    plan: List[str]      # The list of steps generated by the Planner
    current_step: int    # Tracks progress
    results: Dict[str, Any] # Memory of tool outputs
    error: Optional[str] # For robustness

# 2. Define the Planner Node
def planner_node(state: PlanState) -> PlanState:
    print(f"\n[Planner] Generating plan for {state['destination']}...")
    
    system_prompt = "You are a Travel Planner. Output a JSON object with a key 'steps' containing a list of 3 execution steps."
    user_prompt = f"Create a travel plan for {state['destination']} with a budget of ${state['budget']}."
    
    # We request JSON mode to ensure the planner outputs parseable data
    response = invoke_llm(system_prompt, user_prompt, json_mode=True)
    
    try:
        data = json.loads(response)
        # Handle variations in LLM JSON keys
        state['plan'] = data.get('steps', data.get('plan', []))
        print(f"[Planner] Plan created: {state['plan']}")
    except json.JSONDecodeError:
        state['error'] = "Failed to parse plan"
        print("[Planner] Error: Invalid JSON response.")
        
    return state

# 3. Define the Executor Node (ROBUSTNESS ADDED)
def executor_node(state: PlanState) -> PlanState:
    step_idx = state['current_step']
    
    # Check if we are done
    if step_idx >= len(state['plan']):
        print("[Executor] All steps complete.")
        return state
        
    task = state['plan'][step_idx]
    print(f"[Executor] Executing Step {step_idx + 1}: {task}")
    
    try:
        # Simulating a Tool Call with potential failure
        # In real life: tool_output = tools.execute(task)
        
        # Simulate a transient error on the first try of the second step for demo purposes
        if step_idx == 1 and "retry" not in task:
            raise ValueError("Network Timeout (Simulated)")
            
        tool_output = f"Result for {task}: Success (Simulated)"
        time.sleep(0.5) # Simulate latency
        
        # Update State (Success)
        state['results'][task] = tool_output
        state['current_step'] += 1
        
    except ValueError as e:
        # ROBUSTNESS: Catch error, log it, and maybe retry or skip
        print(f"   [Error] {e}. Retrying step...")
        state['plan'][step_idx] = task + " (retry)"
        
    # Recursive call to continue the loop
    return executor_node(state)

# 4. Run Pattern 1
def run_travel_agent():
    print("--- PATTERN 1: ROBUST STATEFUL PLANNING ---")
    initial_state: PlanState = {
        "destination": "Tokyo",
        "budget": 2000,
        "plan": [],
        "current_step": 0,
        "results": {},
        "error": None,
        "final_itinerary": ""
    }
    
    state_after_plan = planner_node(initial_state)
    if not state_after_plan['error']:
        final_state = executor_node(state_after_plan)
        print("Final State Results:", json.dumps(final_state['results'], indent=2))


# =============================================================================
# TOPIC 4: REFLECTION & SELF-CORRECTION
# =============================================================================

def run_reflective_coder():
    print("\n--- PATTERN 2: REFLECTION ---")
    user_req = "Write a python script to calculate the fibonacci sequence up to n"
    state = {"code": "", "errors": ""}
    
    # Max 3 Retries (The "Reflexion" Loop)
    for i in range(3):
        print(f"\n[Loop {i+1}]")
        
        # Step A: Generator
        print(f"[Generator] Drafting code... (Context Errors: '{state['errors']}')")
        
        gen_system = "You are a Python expert. Output ONLY valid python code. No markdown."
        gen_prompt = f"Write code for: {user_req}. Fix these previous errors if any: {state['errors']}"
        
        code_draft = invoke_llm(gen_system, gen_prompt)
        state['code'] = code_draft
        print(f" > Draft generated (Length: {len(code_draft)} chars)")
        
        # Step B: Critic (Using LLM to critique LLM)
        print("[Critic] Validating code...")
        critic_system = "You are a Code Reviewer. Check for syntax errors or logic bugs. Return 'No issues found' if clean, otherwise explain the bug."
        critique = invoke_llm(critic_system, f"Critique this code:\n{code_draft}")
        
        if "No issues found" in critique or "no issues" in critique.lower():
            print(" > Success! Code is valid.")
            print(f"FINAL CODE:\n{state['code']}")
            return state['code']
        else:
            print(f" > Failed: {critique}")
            state['errors'] = critique
            
    return "Failed to generate working code."


# =============================================================================
# TOPIC 5: ROUTING (Often used with SLMs)
# =============================================================================

def run_router_pattern():
    print("\n--- PATTERN 3: ROUTING (FLOW CONTROL) ---")
    tickets = [
        "I need a refund for my subscription",
        "I cannot login to my account",
        "What are your office hours?"
    ]
    
    # System prompt forces specific categories
    ROUTER_SYSTEM = "You are a support router. Classify queries into exactly one category: BILLING, TECH, or GENERAL. Output only the category name."
    
    for ticket in tickets:
        # Note: In production, you might use 'gpt-3.5-turbo' or a fine-tuned SLM here to save cost
        category = invoke_llm(ROUTER_SYSTEM, ticket, model="gpt-4o")
        
        # Cleaning response just in case
        category = category.strip().upper()
        
        if "BILLING" in category:
            print(f"Ticket: '{ticket}' -> Routed to BILLING AGENT")
        elif "TECH" in category:
            print(f"Ticket: '{ticket}' -> Routed to TECH SUPPORT AGENT")
        else:
            print(f"Ticket: '{ticket}' -> Routed to GENERAL INFO AGENT")


# =============================================================================
# TOPIC 6: SLM (Small Language Models) FOR AGENTIC TASKS
# Pattern: Orchestrator-Worker
# =============================================================================

def run_slm_worker_pattern():
    print("\n--- PATTERN 4: SLM ORCHESTRATOR-WORKER ---")
    
    # 1. Orchestrator Plan (Expensive Model: GPT-4o)
    task = "Summarize these 3 user feedback emails about our new feature."
    print(f"[Orchestrator (GPT-4o)] breaking down task: '{task}'")
    
    subtasks = [
        "Email 1: The new feature is great but the UI is a bit slow on mobile.",
        "Email 2: I encountered a bug when trying to export PDF. Fix it.",
        "Email 3: Love the dark mode! Best update yet."
    ]
    
    results = []
    
    # 2. Worker Execution (Cheaper Model: gpt-3.5-turbo)
    # In a real SLM scenario, this might call a local Llama-3-8b via Ollama or Groq
    for i, content in enumerate(subtasks):
        print(f"   [Worker (gpt-3.5-turbo)] Processing chunk {i+1}...")
        
        summary = invoke_llm(
            system_prompt="Summarize this feedback in 1 sentence.", 
            user_prompt=content,
            model="gpt-3.5-turbo" # Using a cheaper/smaller model
        )
        results.append(summary)
        
    print(f"\n[Orchestrator] Aggregated Results:")
    for res in results:
        print(f"- {res}")


# =============================================================================
# MAIN EXECUTION
# =============================================================================
if __name__ == "__main__":
    if client:
        run_travel_agent()
        run_reflective_coder()
        run_router_pattern()
        run_slm_worker_pattern()
    else:
        print("Skipping execution: No OpenAI API Key found.")